%!TEX root = cscw2018-comic.tex

\section{Results}
\label{sec:Results}

\subsection{Raw Data}
\label{sub:Raw Data}
In this section we describe the raw data counts, the number of participants, the number of people whose responses we dropped. The final number of observations

\subsection{Analysis}
\label{sub:Analysis}
We use a Bayesian formulation of the problem of identifying suitable predictors for the messages in comic form.~\textcite{Kay2016} provide an nice introduction on the appropriateness of Bayesian analysis for the HCI community. Bayesian analysis is attractive in our experiment due to two advantages: shifting the conversation from ``did it work'' to ``how strong is the effect''; and benefits to small $n$ studies.

In our experiment, the problem is that of ordinal regression, since the responses are on a Likert scale. There are five independent variables: gesture of the participants in the comic;  distance between the two characters; comic shading; whether the information was positively framed or negatively framed; and whether we presented the comic panel to the left or to the right. The last manipulation to guard against information ordering effects. Thus, we need to estimate the effect on the responses for each of these variables.

A challenge with using ordinal scale: we do not know the ``width'' of each response. That is, while we may know that for example $1<2<3$, we don't know if the range used by subjects to mark ``2'' on the scale, is the same as the range they use for ``1'' and ``3.''  We assume that each response by a subject lies in a continuous metric space, is Normally distributed and the subject then uses thresholds to identify the appropriate ordinal value. The thresholds subjects use to identify each level are unknown and we need to estimate them.


Formally let $r$ be the response of the subjects to the experiment where the comic panel was generated by setting each of the $k$ independent variables $\{x_j\}, j \in [1 \ldots k]$. Then, since we assume that the response variable $r$ is Normally distributed:

\begin{align}
 r      & \sim N(\mu, \sigma)                    \label{eq:response-main}                   \\
 \mu    & \sim \beta_0 + \sum_{i=1}^k \beta_i x_i                        \label{eq:mu-main} \\
 \sigma & \sim hC(L) \label{eq:main-sigma}
\end{align}
\Cref{eq:response-main} says that the response variable $r$ is Normally distributed\footnote{The ``$\sim$'' symbol means that the random variable on the left is drawn from the probability distribuion on the right.} with mean $\mu$ and standard deviation $\sigma$.~\Cref{eq:mu-main} says that the mean response $\mu$ is a linear weighted combination of the predictors.~\Cref{eq:main-sigma} says that the standard deviation of the response is drawn from a half Cauchy ($hC$) distribution with scale parameter $L$, the number of levels in the ordinal scale. We use a seven point Likert scale, so $L=7.$ The half Cauchy distribution, is a fat-tailed distribution, is only defined on nonnegative values, reflects our prior belief in the distribution of $\sigma$. The half Cauchy is a weakly informative prior in that it suggests that while large values of $\sigma$ are possible, they are likely to be smaller. We could have instead defined $\sigma \sim U(0, L)$, an uninformative prior, which implies that $\sigma$ could lie anywhere in that range.

We define the weights $\beta_i$ corresponding to the predictors $x_i$ as follows:
\begin{align}
 \beta_0 & \sim C((1+L)/2, L^2) \\
 \beta_i & \sim  C(0, L^2)
\end{align}

We draw the weights $\beta_i$ from a Cauchy distribution, a weakly informative prior with a fat tail. We normalize the predictors $x_i$ to have a mean of 0 and variance of 1; this normalization (also known as ``centering'') improves inference. The intuition behind the parameters for the Cauchy distribution: the mean of the intercept $\beta_0$ should center around $(1+L)/2=4$, when the predictors have no effect; similarly, $\beta_i, i\neq 0$ should center around 0, when the corresponding predictor has no effect. Notice that we are setting the location (or center) of the Cauchy distribution---due to the scale parameter set to $L^2$, these weights $\beta_i$ can take a wide range of values. As an alternative, we can draw the weights $\beta_i \sim N(0, L^2)$ from a Normal prior; the advantage of a weakly informative prior like the Cauchy distribution is that it allows for greater possibility of outliers in the data in contrast to the Normal distribution~\cite{Kay2016}.

We analyzed the data using PyMC3~\cite{Salvatier2016}, a popular framework for Bayesian inference. Computational techniques for Bayesian inference use a stochastic sampling technique called Markov Chain Monte Carlo (MCMC) that samples the posterior distribution $P(\theta | D)$, where we want to estimate the parameters $\theta$ given the observations $D$. The Gelman-Rubin statistic $\hat{R}$ was around 1, indicating that the different sampling chains converged.
